{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f7fe8e06-39ff-49dd-8358-a80ea843f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scratch/salonso/anaconda3/envs/convnextv2/lib/python3.10/site-packages/MinkowskiEngine-0.5.4-py3.10-linux-x86_64.egg/MinkowskiEngine/__init__.py:36: UserWarning: The environment variable `OMP_NUM_THREADS` not set. MinkowskiEngine will automatically set `OMP_NUM_THREADS=16`. If you want to set `OMP_NUM_THREADS` manually, please export it on the command line before running a python script. e.g. `export OMP_NUM_THREADS=12; python your_program.py`. It is recommended to set it below 24.\n",
      "  warnings.warn(\n",
      "/scratch/salonso/anaconda3/envs/convnextv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import tqdm\n",
    "import gc\n",
    "import glob\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import ListedColormap, BoundaryNorm\n",
    "\n",
    "module_path = os.path.abspath('..')\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "from utils import ini_argparse, split_dataset, collate_test\n",
    "from dataset import *\n",
    "from model import MinkMAEViT, mae_vit_base, mae_vit_large, mae_vit_huge\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from matplotlib import font_manager\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LogNorm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# reset the plot configurations to default\n",
    "plt.rcdefaults()\n",
    "\n",
    "from pathlib import Path\n",
    "font_path = str(Path(matplotlib.get_data_path(), \"fonts/ttf/cmr10.ttf\"))\n",
    "font_manager.fontManager.addfont(font_path)\n",
    "prop = font_manager.FontProperties(fname=font_path)\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = prop.get_name()\n",
    "plt.rcParams[\"axes.formatter.use_mathtext\"] = True\n",
    "params = {'mathtext.default': 'regular' }          \n",
    "plt.rcParams.update(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e5b3c014-14e1-4ccf-9060-02950276b923",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Arguments:\n",
      "  train: False\n",
      "  stage1: True\n",
      "  preprocessing_input: sqrt\n",
      "  preprocessing_output: log\n",
      "  standardize_input: unit-var\n",
      "  standardize_output: unit-var\n",
      "  augmentations_enabled: True\n",
      "  dataset_path: /scratch/salonso/sparse-nns/faser/events_v5.1b\n",
      "  mask_ratio: 0.75\n",
      "  eps: 1e-12\n",
      "  batch_size: 32\n",
      "  epochs: 50\n",
      "  layer_decay: 0.9\n",
      "  num_workers: 32\n",
      "  lr: 0.0001\n",
      "  accum_grad_batches: 1\n",
      "  warmup_steps: 0\n",
      "  cosine_annealing_steps: 0\n",
      "  weight_decay: 0.05\n",
      "  beta1: 0.9\n",
      "  beta2: 0.999\n",
      "  ema_decay: 0.9999\n",
      "  head_init: 0.001\n",
      "  dropout: 0.1\n",
      "  save_dir: /scratch/salonso/sparse-nns/faser/deep_learning/faserDL\n",
      "  name: v1\n",
      "  log_every_n_steps: 50\n",
      "  early_stop_patience: 0\n",
      "  save_top_k: 1\n",
      "  checkpoint_path: /scratch/salonso/sparse-nns/faser/deep_learning/faserDL/checkpoints\n",
      "  checkpoint_name: v1\n",
      "  load_checkpoint: None\n",
      "  gpus: [0]\n",
      "  sets_path: None\n",
      "  load_seg: False\n"
     ]
    }
   ],
   "source": [
    "# manually specify the GPUs to use\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "parser = ini_argparse()\n",
    "args = parser.parse_args([])\n",
    "#args.dataset_path = \"/scratch/salonso/sparse-nns/faser/events_v3.5\"\n",
    "args.dataset_path = \"/scratch/salonso/sparse-nns/faser/events_v5.1b\"\n",
    "args.batch_size = 32\n",
    "args.sets_path = None\n",
    "args.num_workers = 32\n",
    "args.load_seg = False\n",
    "args.stage1 = True\n",
    "args.train = False\n",
    "args.preprocessing_input = \"sqrt\"\n",
    "args.preprocessing_output = \"log\"\n",
    "args.standardize_input = \"unit-var\"\n",
    "args.standardize_output = \"unit-var\"\n",
    "\n",
    "print(\"\\n- Arguments:\")\n",
    "for arg, value in vars(args).items():\n",
    "    print(f\"  {arg}: {value}\")\n",
    "nb_gpus = len(args.gpus)\n",
    "gpus = [int(gpu) for gpu in args.gpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b449ead-f83d-4d43-90fd-20d7e536f5be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Dataset size: 110938 events\n"
     ]
    }
   ],
   "source": [
    "dataset = SparseFASERCALDataset(args)\n",
    "print(\"- Dataset size: {} events\".format(len(dataset)))\n",
    "train_loader, valid_loader, test_loader = split_dataset(dataset, args, splits=[0.6, 0.1, 0.3], test=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8864e357-3d62-4387-9b94-55cbb648a553",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "norm.weight\n",
      "norm.bias\n",
      "1. Downsample Layers     : 1,788,864\n",
      "2. Encoder Blocks        : 85,054,464\n",
      "3. Encoder Misc          : 9,487,104\n",
      "4. Decoder Input         : 406,560\n",
      "5. Decoder Blocks        : 26,818,176\n",
      "6. Decoder Output        : 407,328\n",
      "7. Upsample Layers       : 3,246,048\n",
      "8. Regression Head       : 97\n",
      "9. Classification Head   : 388\n",
      "Other                    : 1,536\n",
      "\n",
      "Total Parameters        : 127,210,565\n",
      "norm.weight\n",
      "norm.bias\n",
      "1. Downsample Layers     : 3,073,644\n",
      "2. Encoder Blocks        : 292,940,928\n",
      "3. Encoder Misc          : 16,322,544\n",
      "4. Decoder Input         : 533,280\n",
      "5. Decoder Blocks        : 26,818,176\n",
      "6. Decoder Output        : 534,288\n",
      "7. Upsample Layers       : 5,590,998\n",
      "8. Regression Head       : 127\n",
      "9. Classification Head   : 508\n",
      "Other                    : 2,016\n",
      "\n",
      "Total Parameters        : 345,816,509\n",
      "norm.weight\n",
      "norm.bias\n",
      "1. Downsample Layers     : 5,071,572\n",
      "2. Encoder Blocks        : 645,511,680\n",
      "3. Encoder Misc          : 26,958,096\n",
      "4. Decoder Input         : 685,344\n",
      "5. Decoder Blocks        : 26,818,176\n",
      "6. Decoder Output        : 686,640\n",
      "7. Upsample Layers       : 9,241,290\n",
      "8. Regression Head       : 163\n",
      "9. Classification Head   : 652\n",
      "Other                    : 2,592\n",
      "\n",
      "Total Parameters        : 714,976,205\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    parts = defaultdict(int)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if not param.requires_grad:\n",
    "            continue\n",
    "        if \"downsample_layers\" in name:\n",
    "            parts[\"1. Downsample Layers\"] += param.numel()\n",
    "        elif \"blocks\" in name and \"decoder_blocks\" not in name:\n",
    "            parts[\"2. Encoder Blocks\"] += param.numel()\n",
    "        elif \"cls_token\" in name or \"pos_embed\" in name or \"global_feats_encoder\" in name:\n",
    "            parts[\"3. Encoder Misc\"] += param.numel()\n",
    "        elif \"decoder_embed\" in name or \"mask_token\" in name or \"decoder_pos_embed\" in name:\n",
    "            parts[\"4. Decoder Input\"] += param.numel()\n",
    "        elif \"decoder_blocks\" in name:\n",
    "            parts[\"5. Decoder Blocks\"] += param.numel()\n",
    "        elif \"decoder_norm\" in name or \"final_embed\" in name:\n",
    "            parts[\"6. Decoder Output\"] += param.numel()\n",
    "        elif \"upsample_layers\" in name:\n",
    "            parts[\"7. Upsample Layers\"] += param.numel()\n",
    "        elif \"reg_head\" in name:\n",
    "            parts[\"8. Regression Head\"] += param.numel()\n",
    "        elif \"cls_head\" in name:\n",
    "            parts[\"9. Classification Head\"] += param.numel()\n",
    "        else:\n",
    "            print(name)\n",
    "            parts[\"Other\"] += param.numel()\n",
    "\n",
    "    total = sum(parts.values())\n",
    "    for k, v in sorted(parts.items(), key=lambda x: x[0]):\n",
    "        print(f\"{k:<25}: {v:,}\")\n",
    "    print(f\"\\nTotal Parameters        : {total:,}\")\n",
    "\n",
    "# Example usage:\n",
    "count_parameters(mae_vit_base())\n",
    "count_parameters(mae_vit_large())\n",
    "count_parameters(mae_vit_huge())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a452d00-931d-47f6-9fac-be8b8f46eb99",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
